{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "1ee38ef4a5a9feb55287fd749643f13d043cb0a7addaab2a9c224cbe137c0062"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['a series of escapades demonstrating the adage that what is good for the goose is also good for the gander some of which occasionally amuses but none of which amounts to much of a story ', 'a series of escapades demonstrating the adage that what is good for the goose', 'a series', 'a', 'series'] [1 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "def load_data(path=\"../train.tsv\", length=500):\n",
    "    df = pd.read_csv(path, sep=\"\\t\")[:length]\n",
    "    df.drop(['PhraseId','SentenceId'], axis=1, inplace=True)\n",
    "    x, y = df[\"Phrase\"].values, df[\"Sentiment\"].values\n",
    "    return (x, y)\n",
    "\n",
    "x, y = load_data(\"../train.tsv\", 500)\n",
    "\n",
    "def clean_text(x):\n",
    "  # Removes special symbols and just keep\n",
    "  # words in lower or upper form\n",
    "  \n",
    "  x = [i.lower() for i in x]\n",
    "  x = [re.sub(r'[^A-Za-z]+', ' ', i) for i in x]\n",
    "  \n",
    "  return x\n",
    "\n",
    "x = clean_text(x)\n",
    "print(x[:5], y[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.legacy.data import Field\n",
    "from torchtext.legacy.data import TabularDataset\n",
    "from torchtext.vocab import GloVe\n",
    "from torchtext.legacy.data import Iterator, BucketIterator\n",
    "import torchtext.datasets\n",
    "from torchtext.datasets import IMDB\n",
    "\n",
    "\n",
    "TEXT = data.Field(tokenize=data.get_tokenizer('spacy'), \n",
    "                  init_token='<SOS>', eos_token='<EOS>',lower=True)\n",
    "\n",
    "train, val, test = data.TabularDataset.splits(\n",
    "        path='../', train='train.tsv', test='test.tsv', format='tsv',\n",
    "        fields=[('Text', TEXT), ('Label', LABEL)])\n",
    "\n",
    "TEXT.build_vocab(train)\n",
    "#TEXT.build_vocab(train, vectors=\"glove.6B.100d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'work': 0, 'for': 1, 'leave': 2, 'companion': 3, 'seeking': 4, 'you': 5, 'wong': 6, 'by': 7, 'should': 8, 'an': 9, 'betrayal': 10, 'have': 11, 'snow': 12, 'intrigue': 13, 'its': 14, 'our': 15, 'who': 16, 'arts': 17, 'chance': 18, 'dialogue': 19, 'fresnadillo': 20, 'performance': 21, 'ismail': 22, 'worth': 23, 'character': 24, 'childhood': 25, 'or': 26, 'love': 27, 'being': 28, 'comedy': 29, 'kong': 30, 'mess': 31, 'oedekerk': 32, 'path': 33, 'does': 34, 'midlife': 35, 'opera': 36, 'epic': 37, 'his': 38, 'joy': 39, 'modest': 40, 'trouble': 41, 'aggressive': 42, 'proves': 43, 'charmer': 44, 'sincere': 45, 'escapades': 46, 'whitewash': 47, 'positively': 48, 'earnest': 49, 'perspective': 50, 'murder': 51, 'thick': 52, 'escapism': 53, 'could': 54, 'us': 55, 'sense': 56, 'dogs': 57, 'be': 58, 'undergoing': 59, 'youth': 60, 'narratively': 61, 'recommend': 62, 'quotations': 63, 'like': 64, 'mr': 65, 's': 66, 'too': 67, 'importance': 68, 'sitting': 69, 'also': 70, 'high': 71, 'considers': 72, 'about': 73, 'throw': 74, 'n': 75, 'baseball': 76, 'self': 77, 'gorgeous': 78, 'from': 79, 'sweet': 80, 'relief': 81, 'despite': 82, 'soap': 83, 'martial': 84, 'mile': 85, 'good': 86, 'mood': 87, 'movie': 88, 'amounts': 89, 'serious': 90, 'quiet': 91, 'entertaining': 92, 'i': 93, 'hard': 94, 'crisis': 95, 'with': 96, 'every': 97, 'none': 98, 'kung': 99, 'in': 100, 'shakespearean': 101, 'series': 102, 'try': 103, 'off': 104, 'something': 105, 'dizzily': 106, 'title': 107, 'absolute': 108, 'a': 109, 'hilarity': 110, 'distort': 111, 'ultimately': 112, 'very': 113, 'movies': 114, 'little': 115, 'there': 116, 'has': 117, 'rooted': 118, 'drama': 119, 'introspective': 120, 'perverse': 121, 'can': 122, 'merchant': 123, 'glorification': 124, 'deceit': 125, 'plays': 126, 'plodding': 127, 'familiar': 128, 'ways': 129, 'extravagant': 130, 'cliched': 131, 'manipulative': 132, 'better': 133, 'inspired': 134, 'story': 135, 'occasionally': 136, 'winning': 137, 'the': 138, 'reading': 139, 'source': 140, 'remain': 141, 'dream': 142, 'combination': 143, 'damned': 144, 'wit': 145, 'are': 146, 'reason': 147, 'realization': 148, 'of': 149, 'what': 150, 'all': 151, 'proportions': 152, 'it': 153, 'pow': 154, 'mythic': 155, 'woman': 156, 'ethnography': 157, 'setting': 158, 'hong': 159, 't': 160, 'day': 161, 'and': 162, 'even': 163, 'thrilling': 164, 'one': 165, 'bartlett': 166, 'is': 167, 'adage': 168, 'bilingual': 169, 'demonstrating': 170, 'time': 171, 'judgment': 172, 'much': 173, 'flick': 174, 'tragedy': 175, 'say': 176, 'mainland': 177, 'juicy': 178, 'sometimes': 179, 'dreams': 180, 'welcome': 181, 'some': 182, 'which': 183, 'just': 184, 'through': 185, 'would': 186, 'fans': 187, 'suspect': 188, 'this': 189, 'to': 190, 'still': 191, 'same': 192, 'independent': 193, 'unless': 194, 'hate': 195, 'amuses': 196, 'but': 197, 'moonlight': 198, 'that': 199, 'less': 200, 'performances': 201, 'so': 202, 'nearly': 203, 'gander': 204, 'goose': 205}\n"
     ]
    }
   ],
   "source": [
    "def build_dict(x):\n",
    "    ret = []\n",
    "    for i in x:\n",
    "        ret += i.split()\n",
    "    \n",
    "    return list(set(ret))\n",
    "\n",
    "d = build_dict(x)\n",
    "\n",
    "word2inx = {d[i]:i for i in range(len(d))}\n",
    "\n",
    "length_dict = len(word2inx)\n",
    "\n",
    "print(word2inx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([109., 102., 149.,  46., 170., 138., 168., 199., 150., 167.,  86.,   1.,\n        138., 205., 167.,  70.,  86.,   1., 138., 204., 182., 149., 183., 136.,\n        196., 197.,  98., 149., 183.,  89., 190., 173., 149., 109., 135.]) 1\n[tensor([109., 102., 149.,  46., 170., 138., 168., 199., 150., 167.,  86.,   1.,\n        138., 205., 167.,  70.,  86.,   1., 138., 204., 182., 149., 183., 136.,\n        196., 197.,  98., 149., 183.,  89., 190., 173., 149., 109., 135.]), tensor([109., 102., 149.,  46., 170., 138., 168., 199., 150., 167.,  86.,   1.,\n        138., 205.])]\n"
     ]
    }
   ],
   "source": [
    "def sentence2vector(sentence):\n",
    "    v = []\n",
    "    for word in sentence.split():\n",
    "        v.append(word2inx[word])\n",
    "    return torch.Tensor(v)\n",
    "\n",
    "test_input = sentence2vector(x[0])\n",
    "test_target = y[0]\n",
    "\n",
    "print(test_input, test_target)\n",
    "\n",
    "def get_batch(x, batch_size=2, point=0):\n",
    "    return  [sentence2vector(i) for i in x[point:point+batch_size]]\n",
    "\n",
    "test_batch = get_batch(x)\n",
    "\n",
    "print(test_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_size=length_dict, embed_dim=100, kernel_wins=[3,4,5], num_class):\n",
    "        super(CNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, embed_dim, max_norm=True)\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, embed_dim, ) for size in kernel_wins])\n",
    "        self.dropout = nn.Dropout(0.6)\n",
    "        self.fc = nn.Linear(len(kernel_wins)*dim_channel, num_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeded_x = self.embedding(x)\n",
    "        embeded_x.unsqueeze(1)\n",
    "        con_x = [conv(emb_x) for conv in self.convs]\n",
    "        pool_x = [F.max_pool1d(x.squeeze(-1), x.size()[2]) for x in con_x]\n",
    "        \n",
    "        fc_x = torch.cat(pool_x, dim=1)\n",
    "        \n",
    "        fc_x = fc_x.squeeze(-1)\n",
    "\n",
    "        fc_x = self.dropout(fc_x)\n",
    "        logit = self.fc(fc_x)\n",
    "        return logit\n",
    "\n",
    "def test_model():\n",
    "    pass\n",
    "\n",
    "test_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}