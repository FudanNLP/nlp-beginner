{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "1ee38ef4a5a9feb55287fd749643f13d043cb0a7addaab2a9c224cbe137c0062"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "from numpy.random import RandomState\n",
    "\n",
    "import torchtext\n",
    "from torchtext.legacy.data import Field\n",
    "from torchtext.legacy.data import TabularDataset\n",
    "from torchtext.vocab import GloVe\n",
    "from torchtext.legacy.data import Iterator, BucketIterator\n",
    "import torchtext.datasets\n",
    "from torchtext.datasets import IMDB\n",
    "\n",
    "import spacy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Split whole dataset into train and valid set\n",
    "df = pd.read_csv('../IMDB_Dataset.csv')\n",
    "rng = RandomState()\n",
    "\n",
    "tr = df.sample(frac=0.7, random_state=rng)\n",
    "tst = df.loc[~df.index.isin(tr.index)]\n",
    "tr.to_csv('train.csv', index=False)\n",
    "tst.to_csv('valid.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Prepare the dataset via torchtext\n",
    "spacy_en = spacy.load('en_core_web_sm', disable=['tagger', 'parser', 'ner', 'textcat'\n",
    "                                     'entity_ruler', 'sentencizer', \n",
    "                                     'merge_noun_chunks', 'merge_entities',\n",
    "                                     'merge_subtokens'])\n",
    "\n",
    "def tokenizer(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "  \n",
    "# set up fields\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip()\n",
    "  \n",
    "\n",
    "#Creating field for text and label\n",
    "TEXT = Field(sequential=True, tokenize=tokenizer, lower=True)\n",
    "LABEL = Field(sequential=False)\n",
    "\n",
    "#clean the text\n",
    "TEXT.preprocessing = torchtext.legacy.data.Pipeline(clean_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datafield = [('text', TEXT),  ('label', LABEL)]\n",
    "train = TabularDataset(path ='./train.csv',  \n",
    "                             format='csv',\n",
    "                             skip_header=True,\n",
    "                             fields=train_datafield)\n",
    "\n",
    "\n",
    "#%%\n",
    "test_datafield = [('text', TEXT),  ('label',LABEL)]\n",
    "\n",
    "test = TabularDataset(path ='./valid.csv', \n",
    "                       format='csv',\n",
    "                       skip_header=True,\n",
    "                       fields=test_datafield)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['when', 'i', 'think', \"'\", 'women', 'in', 'prison', \"'\", ',', 'my', 'mind', 'often', 'goes', 'to', 'sleazy', 'italian', '', 'spanish', 'productions', 'by', 'directors', 'such', 'as', 'jess', 'franco', 'and', 'bruno', 'mattei', '', 'and', 'while', 'these', 'films', 'are', 'often', 'very', 'sleazy', ',', 'they', \"'re\", 'also', 'very', 'samey', 'and', 'once', 'you', \"'ve\", 'seen', 'one', '', 'you', 'might', 'as', 'well', 'have', 'seen', 'them', 'all', '', 'i', 'have', 'to', 'admit', 'that', 'these', 'types', 'of', 'films', 'generally', 'are', \"n't\", 'my', 'favourites', '', 'but', 'in', 'fact', 'the', 'idea', 'of', 'women', 'behind', 'bars', 'has', 'been', 'done', 'very', 'well', 'on', 'several', 'occasions', 'outside', 'of', 'italy', 'and', 'spain', '', 'and', 'roger', 'corman', \"'s\", 'new', 'world', 'pictures', 'is', 'responsible', 'for', 'some', 'of', 'the', 'best', 'of', 'them', '', 'caged', 'heat', 'is', 'the', 'directorial', 'debut', 'of', 'oscar', '', 'winning', 'director', 'jonathan', 'demme', ',', 'and', 'it', \"'s\", 'a', 'well', 'done', 'little', 'flick', 'with', 'plenty', 'of', 'entertainment', 'value', '!', 'naturally', ',', 'the', 'film', 'centres', 'on', 'the', 'story', 'of', 'a', 'girl', 'who', 'is', 'caught', 'committing', 'crime', 'and', 'sent', 'to', 'a', 'women', \"'s\", \"'\", 'prison', 'where', 'she', 'is', 'introduced', 'to', 'a', 'host', 'of', 'violent', 'inmates', '', 'this', 'prison', 'is', 'ruled', 'over', 'by', 'the', 'stuff', 'wheelchair', 'bound', 'superintendent', 'mcqueen', '', 'and', 'she', 'takes', 'offence', 'to', 'a', 'play', 'put', 'on', 'by', 'the', 'girls', '', 'leading', 'them', 'to', 'plot', 'an', 'escape br', 'br', 'this', 'film', 'is', 'much', 'lighter', 'on', 'the', 'sleaze', 'than', 'i', \"'m\", 'used', 'to', 'in', 'a', 'women', 'in', 'prison', 'flick', '', 'but', 'this', 'is', 'more', 'than', 'compensated', 'for', 'by', 'some', 'great', 'action', 'scenes', 'and', 'dialogue', 'and', 'that', \"'s\", 'what', 'ensures', 'caged', 'heat', 'entertains', 'throughout', '', 'it', 'does', 'have', 'to', 'be', 'said', 'that', 'the', 'plot', 'is', 'not', 'particularly', 'original', 'or', 'ambitious', 'and', 'basically', 'follows', 'a', 'structure', 'similar', 'to', 'many', 'other', 'women', 'in', 'prison', 'films', 'that', 'came', 'before', 'it', '', 'but', 'that', \"'s\", 'not', 'such', 'a', 'big', 'problem', '', 'the', 'film', 'never', 'gets', 'boring', 'and', 'is', 'peppered', 'with', 'standout', 'scenes', '', 'including', 'an', 'escape', 'attempt', 'while', 'out', 'working', 'in', 'a', 'field', 'and', 'a', 'bank', 'robbery', '', 'the', 'film', 'is', 'helped', 'along', 'by', 'assured', 'direction', 'from', 'the', 'man', 'who', 'would', 'go', 'on', 'to', 'helm', 'the', 'masterpiece', 'the', 'silence', 'of', 'the', 'lambs', 'and', 'a', 'great', 'cast', 'with', 'plenty', 'of', 'standouts', '', 'including', 'best', 'of', 'all', 'the', 'legendary', 'barbara', 'steele', 'in', 'the', 'role', 'of', 'the', 'head', 'prison', 'warden', '', 'overall', ',', 'caged', 'heat', 'may', 'not', 'leave', 'the', 'viewer', 'with', 'much', 'to', 'think', 'about', 'by', 'the', 'end', '', 'but', 'it', \"'s\", 'a', 'brilliantly', 'entertaining', 'little', 'grindhouse', 'flick', 'and', 'anyone', 'that', 'enjoys', 'this', 'type', 'of', 'film', 'will', 'surely', 'want', 'to', 'track', 'it', 'down', ''] positive\n['a', 'wonderful', 'little', 'production', '', '', 'br', 'br', 'the', 'filming', 'technique', 'is', 'very', 'unassuming', 'very', 'old', '', 'time', '', 'bbc', 'fashion', 'and', 'gives', 'a', 'comforting', ',', 'and', 'sometimes', 'discomforting', ',', 'sense', 'of', 'realism', 'to', 'the', 'entire', 'piece', '', '', 'br', 'br', 'the', 'actors', 'are', 'extremely', 'well', 'chosen', 'michael', 'sheen', 'not', 'only', '', 'has', 'got', 'all', 'the', 'polari', '', 'but', 'he', 'has', 'all', 'the', 'voices', 'down', 'pat', 'too', '!', 'you', 'can', 'truly', 'see', 'the', 'seamless', 'editing', 'guided', 'by', 'the', 'references', 'to', 'williams', \"'\", 'diary', 'entries', ',', 'not', 'only', 'is', 'it', 'well', 'worth', 'the', 'watching', 'but', 'it', 'is', 'a', 'terrificly', 'written', 'and', 'performed', 'piece', '', 'a', 'masterful', 'production', 'about', 'one', 'of', 'the', 'great', 'master', \"'s\", 'of', 'comedy', 'and', 'his', 'life', '', '', 'br', 'br', 'the', 'realism', 'really', 'comes', 'home', 'with', 'the', 'little', 'things', '', 'the', 'fantasy', 'of', 'the', 'guard', 'which', ',', 'rather', 'than', 'use', 'the', 'traditional', \"'\", 'dream', \"'\", 'techniques', 'remains', 'solid', 'then', 'disappears', '', 'it', 'plays', 'on', 'our', 'knowledge', 'and', 'our', 'senses', ',', 'particularly', 'with', 'the', 'scenes', 'concerning', 'orton', 'and', 'halliwell', 'and', 'the', 'sets', '\\\\(', 'particularly', 'of', 'their', 'flat', 'with', 'halliwell', \"'s\", 'murals', 'decorating', 'every', 'surface', '\\\\)', 'are', 'terribly', 'well', 'done', ''] positive\n"
     ]
    }
   ],
   "source": [
    "#%%Show some example to show the dataset\n",
    "print(train[0].text,  train[0].label)\n",
    "print(test[0].text,  test[0].label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      ".vector_cache/glove.6B.zip: 862MB [06:51, 2.09MB/s]                           \n",
      "100%|█████████▉| 399999/400000 [00:38<00:00, 10311.07it/s]\n"
     ]
    }
   ],
   "source": [
    "#%% Check the dataset\n",
    "TEXT.build_vocab(train, vectors= 'glove.6B.300d')\n",
    "LABEL.build_vocab(train)\n",
    "#%% load the pretrained embedding\n",
    "vocab = TEXT.vocab\n",
    "\n",
    "#%% Create the Iterator for datasets (Iterator works like dataloader)\n",
    "\n",
    "train_iter = Iterator(\n",
    "        train, \n",
    "        batch_size=64,\n",
    "        device=torch.device('cuda'), \n",
    "        sort_within_batch=False,\n",
    "        repeat=False)\n",
    "\n",
    "test_iter = Iterator(test, batch_size=64, device=torch.device('cuda'), \n",
    "                     sort_within_batch=False, repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Text CNN model\n",
    "class textCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_built, emb_dim, dim_channel, kernel_wins, num_class):\n",
    "        super(textCNN, self).__init__()\n",
    "        #load pretrained embedding in embedding layer.\n",
    "        self.embed = nn.Embedding(len(vocab_built), emb_dim)\n",
    "        self.embed.weight.data.copy_(vocab_built.vectors)\n",
    "    \n",
    "        #Convolutional Layers with different window size kernels\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, dim_channel, (w, emb_dim)) for w in kernel_wins])\n",
    "        #Dropout layer\n",
    "        self.dropout = nn.Dropout(0.6)\n",
    "        \n",
    "        #FC layer\n",
    "        self.fc = nn.Linear(len(kernel_wins)*dim_channel, num_class)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        emb_x = self.embed(x)\n",
    "        emb_x = emb_x.unsqueeze(1)\n",
    "\n",
    "        con_x = [conv(emb_x) for conv in self.convs]\n",
    "\n",
    "        pool_x = [F.max_pool1d(x.squeeze(-1), x.size()[2]) for x in con_x]\n",
    "        \n",
    "        fc_x = torch.cat(pool_x, dim=1)\n",
    "        \n",
    "        fc_x = fc_x.squeeze(-1)\n",
    "\n",
    "        fc_x = self.dropout(fc_x)\n",
    "        logit = self.fc(fc_x)\n",
    "        return logit\n",
    "        \n",
    "\n",
    "#%% Training the Model\n",
    "def train(model, device, train_itr, optimizer, epoch, max_epoch):\n",
    "    model.train()\n",
    "    corrects, train_loss = 0.0,0\n",
    "    for batch in train_itr:\n",
    "        text, target = batch.text, batch.label\n",
    "        text = torch.transpose(text,0, 1)\n",
    "        target.data.sub_(1)\n",
    "        text, target = text.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logit = model(text)\n",
    "        \n",
    "        loss = F.cross_entropy(logit, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss+= loss.item()\n",
    "        result = torch.max(logit,1)[1]\n",
    "        corrects += (result.view(target.size()).data == target.data).sum()\n",
    "    \n",
    "    size = len(train_itr.dataset)\n",
    "    train_loss /= size \n",
    "    accuracy = 100.0 * corrects/size\n",
    "  \n",
    "    return train_loss, accuracy\n",
    "    \n",
    "def valid(model, device, test_itr):\n",
    "    model.eval()\n",
    "    corrects, test_loss = 0.0,0\n",
    "    for batch in test_itr:\n",
    "        text, target = batch.text, batch.label\n",
    "        text = torch.transpose(text,0, 1)\n",
    "        target.data.sub_(1)\n",
    "        text, target = text.to(device), target.to(device)\n",
    "        \n",
    "        logit = model(text)\n",
    "        loss = F.cross_entropy(logit, target)\n",
    "\n",
    "        \n",
    "        test_loss += loss.item()\n",
    "        result = torch.max(logit,1)[1]\n",
    "        corrects += (result.view(target.size()).data == target.data).sum()\n",
    "    \n",
    "    size = len(test_itr.dataset)\n",
    "    test_loss /= size \n",
    "    accuracy = 100.0 * corrects/size\n",
    "    \n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "textCNN(\n  (embed): Embedding(111395, 300)\n  (convs): ModuleList(\n    (0): Conv2d(1, 100, kernel_size=(3, 300), stride=(1, 1))\n    (1): Conv2d(1, 100, kernel_size=(4, 300), stride=(1, 1))\n    (2): Conv2d(1, 100, kernel_size=(5, 300), stride=(1, 1))\n  )\n  (dropout): Dropout(p=0.6, inplace=False)\n  (fc): Linear(in_features=300, out_features=2, bias=True)\n)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-311c3653b5f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m#train loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train Epoch: {} \\t Loss: {} \\t Accuracy: {}%'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-788cf78a47c1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_itr, optimizer, epoch, max_epoch)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mcorrects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_itr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/torchtext/legacy/data/iterator.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    158\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m                         \u001b[0mminibatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/torchtext/legacy/data/batch.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, dataset, device)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfield\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                     \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/torchtext/legacy/data/field.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, batch, device)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \"\"\"\n\u001b[1;32m    230\u001b[0m         \u001b[0mpadded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumericalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/torchtext/legacy/data/field.py\u001b[0m in \u001b[0;36mnumericalize\u001b[0;34m(self, arr, device)\u001b[0m\n\u001b[1;32m    351\u001b[0m                 \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m         \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequential\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_first\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \"multiprocessing, you must use the 'spawn' start method\")\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_cuda_getDeviceCount'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             raise AssertionError(\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "model = textCNN(vocab, 300, 100, [3, 4 , 5] , 2)\n",
    "# print the model summery\n",
    "print(model)    \n",
    "    \n",
    "train_loss = []\n",
    "train_acc = []\n",
    "test_loss = []\n",
    "test_acc = []\n",
    "best_test_acc = -1\n",
    "\n",
    "# Use GPU if it is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "\n",
    "#optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(1, 10+1):\n",
    "    #train loss\n",
    "    tr_loss, tr_acc = train(model, device, train_iter, optimizer, epoch, 100)\n",
    "    print('Train Epoch: {} \\t Loss: {} \\t Accuracy: {}%'.format(epoch, tr_loss, tr_acc))\n",
    "    \n",
    "    ts_loss, ts_acc = valid(model, device, test_iter)\n",
    "    print('Valid Epoch: {} \\t Loss: {} \\t Accuracy: {}%'.format(epoch, ts_loss, ts_acc))\n",
    "    \n",
    "    if ts_acc > best_test_acc:\n",
    "        best_test_acc = ts_acc\n",
    "        #save paras(snapshot)\n",
    "        print(\"model saves at {} accuracy\".format(best_test_acc))\n",
    "        torch.save(model.state_dict(), \"textCNN_IMDB_best_valid\")\n",
    "        \n",
    "    train_loss.append(tr_loss)\n",
    "    train_acc.append(tr_acc)\n",
    "    test_loss.append(ts_loss)\n",
    "    test_acc.append(ts_acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot train/validation loss versus epoch\n",
    "for i in range(0):\n",
    "    x = list(range(1, 10+1))\n",
    "    plt.figure()\n",
    "    plt.title(\"train/validation loss versus epoch\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"Average loss\")\n",
    "    plt.plot(x, train_loss,label=\"train loss\")\n",
    "    plt.plot(x, test_loss, color='red', label=\"test loss\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    #plot train/validation accuracy versus epoch\n",
    "    x = list(range(1, 10+1))\n",
    "    plt.figure()\n",
    "    plt.title(\"train/validation accuracy versus epoch\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"accuracy(%)\")\n",
    "    plt.plot(x, train_acc,label=\"train accuracy\")\n",
    "    plt.plot(x, test_acc, color='red', label=\"test accuracy\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}